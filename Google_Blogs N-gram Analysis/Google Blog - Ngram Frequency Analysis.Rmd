---
title: "Frequency Analysis on N-gram phrases"
output:
  pdf_document: default
  html_document: default
---

```{r}
# Read and parse HTML file
library("XML")
library("RCurl")
```


# Importing data from website

```{r}
doc.url1<-getURL(
"https://www.blog.google/technology/safety-security/safety-center-helping-you-stay-safe-online/")
doc.html1 <- htmlParse(doc.url1)

# Extract all the paragraphs (HTML tag is p, starting at
# the root of the document). Unlist flattens the list to
# create a character vector.

plain.text1 <- xpathApply(doc.html1, "//div[@class='h-c-grid']/div[@class='uni-paragraph
          h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6
          h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3']", 
          xmlValue)
cat(paste(plain.text1, collapse = " "),file="Corpus/outfile1.txt")
actual.text1<-paste(readLines("Corpus/outfile1.txt"),collapse=" ")
```

```{r}
doc.url2<-getURL(
  "https://www.blog.google/products/google-cloud/new-security-tools-to-help-improve/")
doc.html2 <- htmlParse(doc.url2)

# Extract all the paragraphs (HTML tag is p, starting at
# the root of the document). Unlist flattens the list to
# create a character vector.
plain.text2 <- xpathApply(doc.html2, "//div[@class='h-c-grid']/div[@class='uni-paragraph
          h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6
          h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3']", 
          xmlValue)
cat(paste(plain.text2, collapse = " "),file="Corpus/outfile2.txt")
actual.text2<-paste(readLines("Corpus/outfile2.txt"),collapse=" ")
```

```{r}
doc.url3<-getURL("https://www.blog.google/products/google-cloud/bolstering-security-across-google-cloud/")
doc.html3 <- htmlParse(doc.url3)

# Extract all the paragraphs (HTML tag is p, starting at
# the root of the document). Unlist flattens the list to
# create a character vector.
plain.text3 <- xpathApply(doc.html3, "//div[@class='h-c-grid']/div[@class='uni-paragraph
          h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6
          h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3']", 
          xmlValue)
cat(paste(plain.text3, collapse = " "),file="Corpus/outfile3.txt")
actual.text3<-paste(readLines("Corpus/outfile3.txt"),collapse=" ")
```


# Cleaning text data in R

```{r}
library(tm)
```

# To find out the path of the destination folder
##```{r}
##file.choose()
##```



# To retrieve only the txt files from the destinaton folder

```{r}
folder<- "C:\\Users\\roopa\\Documents\\Corpus"
list.files(path=folder)
filelist<-list.files(path=folder,pattern="*.txt")
```

# Building the corpus

```{r}
filelist<-paste(folder, "\\", filelist, sep="")
typeof(filelist)
a<-lapply(filelist, FUN=readLines)
corpus<-lapply(a, FUN=paste, collapse=" ")
```

# Cleaning the corpus`

```{r}
# Removing spaces and punctuations
clean.corpus<- gsub(pattern="\\W", replace=" ", corpus)

# Removing digits
clean.corpus<- gsub("\\d", replace=" ", clean.corpus)

# converting all words to lowercase
clean.corpus<- tolower(clean.corpus)

# removing stopwords
clean.corpus<- removeWords(clean.corpus, stopwords("english"))

# removing single length words
# if we want to remove words starting with a 
#particular alphabet, say 'd', then write gsub("\\bd\\b, replace=" ", clean.text)

# if we want to remove words starting with a particular alphabet, 
#say 'd' of length 1, then write gsub("\\bd\\b{1},replace=" ",clean.text)

# Similarly for removing words starting with a set of alphabets
#(say: d,a and s), write \\b[c('d','a','s')]\\b inside gsub

#Here, we are removing only single letter words
clean.corpus<- gsub("\\b[A-z]\\b{1}",replace=" ", clean.corpus)

#Removing extra whitespaces
clean.corpus<- stripWhitespace(clean.corpus)
```


```{r}
library(stringr)
library(wordcloud)
```



# Creating wordcloud

```{r}
wordcloud(clean.corpus,random.order=FALSE, scale=c(3,0.5),color=rainbow(3))
real.corpus<-Corpus(VectorSource(clean.corpus))
real.corpus
```

# Creating Document Matrix

```{r}
doc.matrix<-TermDocumentMatrix(real.corpus)
doc.matrix
matrixformat<-as.matrix(doc.matrix)
```
```{r}
colnames(matrixformat)<-c("Doc1","Doc2","Doc3")
comparison.cloud(matrixformat,random.order=FALSE,scale=c(1,.5),
                 max.words = 100,title.size=1,match.colors=TRUE)
```

# Sentiment analysis

```{r}

#Identify working directory and copy paste the sentiments words to text files 
#and place it in that directory
getwd()
pos.text<-scan('positive_sentiments.txt',what='character',comment.char=';')
neg.text<-scan('negative_sentiments.txt', what='character',comment.char=';')

#to convert to bag or list of words
clean.corpus.bag <- str_split(clean.corpus,pattern="\\s+")
clean.corpus.bag
```

```{r}
#Finding matching positive word count
lapply(clean.corpus.bag, function(x){sum(!is.na(match(x,pos.text)))})

#Finding matching negative word count
lapply(clean.corpus.bag, function(x){sum(!is.na(match(x,neg.text)))})

#Finding total sentiment score
score<-unlist(lapply(clean.corpus.bag, 
       function(x){sum(!is.na(match(x,pos.text)))- sum(!is.na(match(x,neg.text)))}))
score

# mean of sentiment scores
mean(score)

# standard deviation of sentiment score
sd(score)

# Histogram for sentiment scores
hist(score)
```

# Unigram Tokenization

```{r}
library(rJava)
library(RWeka)
library(ggplot2)

# Use Weka’s n-gram tokenizer to create a TDM 
# that uses as terms the unigrams that appear in the corpus.

Unigram_Tokenizer <- function(x){
  NGramTokenizer(x, Weka_control(min=1, max=1))
}


#create a matrix
tdm <- TermDocumentMatrix(real.corpus, control = list(tokenize = Unigram_Tokenizer))


# Extract the frequency of each unigram and analyse the twenty most frequent ones.

freq <-sort(rowSums(as.matrix(tdm)),decreasing = TRUE)
freq.df <- data.frame(word=names(freq), freq=freq)
head(freq.df, 20)



# Plotting the Bigram model

ggplot(head(freq.df,20), aes(reorder(word,freq), freq, fill=freq)) +   
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Unigrams") + ylab("Frequency") +  scale_fill_continuous(type="gradient")+
  ggtitle("Most frequent Unigrams")
```

# N gram analysis

# Bigram Tokenization

```{r}
library(rJava)
library(RWeka)
library(ggplot2)

# Use Weka’s n-gram tokenizer to create a TDM 
# that uses as terms the bigrams that appear in the corpus.

Bigram_Tokenizer <- function(x){
  NGramTokenizer(x, Weka_control(min=2, max=2))
}


#create a matrix
tdm <- TermDocumentMatrix(real.corpus, control = list(tokenize = Bigram_Tokenizer))


# Extract the frequency of each bigram and analyse the twenty most frequent ones.

freq <-sort(rowSums(as.matrix(tdm)),decreasing = TRUE)
freq.df <- data.frame(word=names(freq), freq=freq)
head(freq.df, 20)



# Plotting the Bigram model

ggplot(head(freq.df,20), aes(reorder(word,freq), freq, fill=freq)) +   
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Bigrams") + ylab("Frequency") +  scale_fill_continuous(type="gradient")+ 
  scale_colour_hue(h = c(90, 180))+
  ggtitle("Most frequent Bigrams")
```

# Trigram Tokenizer

```{r}

# Use Weka’s n-gram tokenizer to create a TDM 
# that uses as terms the trigrams that appear in the corpus.

Trigram_Tokenizer <- function(x){
  NGramTokenizer(x, Weka_control(min=3, max=3))
}


#create a matrix
tdm <- TermDocumentMatrix(real.corpus, control = list(tokenize = Trigram_Tokenizer))


# Extract the frequency of each trigram and analyse the twenty most frequent ones.

freq <-sort(rowSums(as.matrix(tdm)),decreasing = TRUE)
freq.df <- data.frame(word=names(freq), freq=freq)
head(freq.df, 20)

# Plotting the Trigram model

ggplot(head(freq.df,20), aes(reorder(word,freq), freq,fill=freq)) +   
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Trigrams") + ylab("Frequency") +   scale_fill_continuous(type="viridis")+ 
  scale_colour_hue(h = c(90, 180))+
  ggtitle("Most frequent trigrams")
```



```{r}
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tdm.trigram = TermDocumentMatrix(real.corpus,
control = list(tokenize = TrigramTokenizer))

freq = sort(rowSums(as.matrix(tdm.trigram)),decreasing = TRUE)
freq.df = data.frame(word=names(freq), freq=freq)
head(freq.df, 20)


```
